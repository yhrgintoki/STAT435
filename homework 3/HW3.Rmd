---
title: "STAT435 hw3"
author: "Haoran Yu"
date: "April 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

####(a,b)

![](Problem 1.jpg)

####(c)

```{r}
curve(dexp(x, 2), from = 0, to = 2.5, ylab = "pdf", col = "blue")
curve(dexp(x, 7), add = T, col = "red")
abline(v = log(2/7)/(2-7), lty = 2)
legend('topright', c("class 1", "class 2", "Bayes decision boundary"), col = c("blue", "red", "black"), lty = c(1,1,2))
```

The dashed line corresponds to the Bayes classifier decision boundary.

The Bayes classifier will assign an observation to class 1 if the observation is on the right of the dashed line.

The Bayes classifier will assign an observation to class 2 if the observation is on the left of the dashed line.

####(d,e)

See attatched file above

## Problem 2

![](Problem 2.jpg)

## Problem 3

####(a)

On average, we will use 10% of the available observations to make the prediction.

####(b)

$10\% *10\%=1\%$

On average, we will use 1% of the available observations to make the prediction.

####(c)

$10\%^{100}=\frac{1}{10^{98}}\%$

On average, we will use $\frac{1}{10^{98}}\%$ of the available observations to make the prediction.

####(d)

As seen in previous part, the fraction of the available observations that we can use to make the prediction decreases exponentially by number of predictiors.

####(e)

$p=1,l=0.1$

$p=2,l=0.1^{1/2}$

$p=3,l=0.1^{1/3}$

...

$p=100,l=0.1^{1/100}$

The length of each side of the hypercube is $0.1^{1/n}$ when p is n.

## Problem 4

####(a)

```{r}
library(ISLR)
data(Default)
nrow(Default)
ncol(Default)
```

The value of n is 10000 and the value of p is 3 (3 predictors and 1 response).

I am trying to predict whether the customer will default on their debt.

The first feature is student, which means whether the customer is a student.

The second feature is balance, which means the average balance that the customer has remaining on their credit card after making their monthly payment.

The third feature is income, which means the income of customer.

####(b)

```{r}
train = Default[1:8000, ]
test = Default[8001:10000, ]
library (MASS)
lda.fit = lda(default ~ as.factor(student) + balance + income, data = train)
lda.pred = predict(lda.fit)
mean(lda.pred$class != train$default)
lda.pred = predict(lda.fit, test)
mean(lda.pred$class != test$default)
```

The training error of the model obtain is 0.027875, the test error of the model obtain is 0.026

####(c)

```{r}
qda.fit = qda(default ~ as.factor(student) + balance + income, data = train)
qda.pred = predict(qda.fit)
mean(qda.pred$class != train$default)
qda.pred = predict(qda.fit, test)
mean(qda.pred$class != test$default)
```

The training error of the model obtain is 0.027375, the test error of the model obtain is 0.0245

####(d)

```{r}
dummy = numeric(8000)
for(i in 1:8000){
  if(train$student[i]=="Yes")
    dummy[i]=1
}
glm.fit = glm(default ~ student + balance + income, data = train, family = binomial)
glm.prob = predict(glm.fit, type = "response")
glm.pred = rep("No", 8000)
glm.pred[glm.prob > 0.5] = "Yes"
mean(glm.pred != train$default)
glm.prob = predict(glm.fit, test, type = "response")
glm.pred = rep("No", 2000)
glm.pred[glm.prob > 0.5] = "Yes"
mean(glm.pred != test$default)
```

The training error of the model obtain is 0.026625, the test error of the model obtain is 0.0265

####(e)

```{r}
library(class)
train.obs = train[, 1]
test.obs = test[, 1]
dummy = numeric(8000)
for(i in 1:8000){
  if(train$student[i] == "Yes") 
    dummy[i] = 1
}
train.dat = cbind(dummy, train[, 3:4])
knn.pred = knn(train.dat, train.dat, train.obs, k = 1)
mean(knn.pred != train.obs)
dummy = numeric(2000)
for(i in 1:2000){
  if(test$student[i] == "Yes") 
    dummy[i] = 1
}
test.dat = cbind(dummy, test[, 3:4])
knn.pred = knn(train.dat, test.dat, train.obs, k = 1)
mean(knn.pred != test.obs)
```

The training error of the model obtain is 0, the test error of the model obtain is 0.0435

####(f)

The LDA and logistic regression provide nearly the same training error and test error, and QDA gives a smaller training error and test error than LDA, while knn gives zero in training error and much bigger test error than the other three method.
